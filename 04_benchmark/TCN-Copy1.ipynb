{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vTDHog-dM_NL"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import argparse\n",
    "import glob\n",
    "from tensorboardX import SummaryWriter\n",
    "import pandas as pd\n",
    "\n",
    "import pathlib\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from utils import data_generator\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from torch.nn.modules.module import _addindent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E634Ocm-N5j0"
   },
   "outputs": [],
   "source": [
    "def parse_command_line():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-n', '--net', type=str, default='simpleLSTM', help='task to be trained')\n",
    "    parser.add_argument('-f', '--file', type=str, default='simpleLSTM', help='tensorboard location')\n",
    "    parser.add_argument('-r', '--runs', type=str, default='test/simpleLSTM', help='tensorboard location')\n",
    "    parser.add_argument('-b', '--batchsize', type=int, default=64, help='batchsize')\n",
    "    parser.add_argument('-m', '--max', type=int, default=20, help='batchsize')\n",
    "    parser.add_argument('-l', '--force_learning_rate', type=float, default=0.00001, help='setting learning rate')\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Srz8zJtmOCDs"
   },
   "outputs": [],
   "source": [
    "opt = parse_command_line()\n",
    "writer = SummaryWriter(opt.runs)\n",
    "params = { 'batch_size': opt.batchsize, 'shuffle': True, 'num_workers': 10, 'drop_last': True}\n",
    "learning_rate = opt.force_learning_rate\n",
    "\n",
    "data = {'Data_hz': 2, 'Frame_len': 25}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "loAFquISOIQj"
   },
   "outputs": [],
   "source": [
    "def get_filename_type(file):\n",
    "    filename = file.split(\"/\")[-1].split('.')[:-1]\n",
    "    file_type = file.split(\"/\")[-1].split('.')[-1]\n",
    "    return filename, file_type\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCHGjY68ON1d"
   },
   "outputs": [],
   "source": [
    "class my_dataset(Dataset):\n",
    "    def __init__(self, csv_path_folder, npy_path_folder, data_hz, frame_len):\n",
    "        \n",
    "        self.data_hz = data_hz\n",
    "        self.frame_len = frame_len\n",
    "\n",
    "        # txt\n",
    "        self.csv_filenames = sorted(glob.glob(csv_path_folder))\n",
    "        self.csv_list_of_dfs = [np.loadtxt(filename, dtype=np.long) for filename in self.csv_filenames]\n",
    "        self.csv_dataframes = {}\n",
    "        self.csv_filename = []\n",
    "        self.csv_result = []\n",
    "        for csv_dataframe, csv_filename in zip(self.csv_list_of_dfs, self.csv_filenames):\n",
    "            tmp_name,_= get_filename_type(csv_filename)\n",
    "            self.csv_filename.append(tmp_name)\n",
    "            self.csv_dataframes[csv_filename] = csv_dataframe\n",
    "        for i in self.csv_list_of_dfs:\n",
    "            for j in range(len(i)-(self.frame_len-1)*self.data_hz):\n",
    "                tmp_list=[]\n",
    "                for k in range(self.frame_len):\n",
    "                    tmp_list.append(i[j+k*self.data_hz].argmax(axis=0))\n",
    "                    #tmp_list.append(i[j+k*self.data_hz])\n",
    "                self.csv_result.append(tmp_list[-1])\n",
    "        self.csv_conbined_df = np.concatenate(self.csv_list_of_dfs)\n",
    "        self.csv_torch_tensor = torch.tensor(self.csv_conbined_df)\n",
    "        print(self.csv_result[0])\n",
    "        # npy\n",
    "        self.npy_filenames = sorted(glob.glob(npy_path_folder))\n",
    "        self.npy_list_of_frames = [np.load(filename) for filename in self.npy_filenames]\n",
    "        self.npy_inputs = {}\n",
    "        self.npy_filename = []\n",
    "        self.npy_result = []\n",
    "\n",
    "        for i in self.npy_list_of_frames:\n",
    "            for j in range(len(i)-(self.frame_len-1)*self.data_hz):\n",
    "                tmp_list=[]\n",
    "                for k in range(self.frame_len):\n",
    "                    tmp_list.append(np.concatenate(i[j+k*self.data_hz]))\n",
    "                self.npy_result.append(tmp_list)\n",
    "\n",
    "        for npy_input, npy_filename in zip(self.npy_list_of_frames, self.npy_filenames):\n",
    "            tmp_name,_= get_filename_type(npy_filename)\n",
    "            if tmp_name not in self.csv_filename:\n",
    "                self.npy_inputs[npy_filename] = npy_input\n",
    "        self.npy_conbined_inputs = np.concatenate(self.npy_list_of_frames, axis=0, out=None)\n",
    "        self.npy_torch_tensor = torch.tensor(self.npy_conbined_inputs)\n",
    "        \n",
    "        print(\"length of input skeleton is:\"+str(len(self.npy_conbined_inputs))+\" mod of batch size is:\"+str(len(self.npy_conbined_inputs)%params['batch_size']))\n",
    "        print(\"length of input label is:\"+str(len(self.csv_conbined_df))+\" mod of batch size is:\"+str(len(self.csv_conbined_df)%params['batch_size']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv_result)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return np.asarray(self.npy_result[index]),np.asarray(self.csv_result[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ogCNSI__OW8B"
   },
   "outputs": [],
   "source": [
    "class BlockLSTM(nn.Module):\n",
    "    def __init__(self, time_steps, num_variables, num_classes, lstm_hs=256, dropout=0.8, attention=False):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=time_steps, hidden_size=lstm_hs, num_layers=num_variables)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        #self.fc = nn.Linear(lstm_hs, num_classes)\n",
    "        self.dense = nn.Linear(lstm_hs, num_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) #nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        # (batch_size, num_variables, time_steps), e.g. (128, 1, 512)\n",
    "        # x = torch.transpose(x, 0, 1)\n",
    "        # (num_variables, batch_size, time_steps)\n",
    "        x,_ = self.lstm(x)\n",
    "        # dropout layer input shape:\n",
    "        # y = self.dropout(x)\n",
    "        # output shape is of the form ()\n",
    "        #y = self.fc(x)\n",
    "        x = self.dense(x)\n",
    "        #x = torch.squeeze(x)\n",
    "        # pass through Softmax activation\n",
    "        y = self.softmax(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from tcn import TemporalConvNet\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(TCN, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        #self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "        self.dense = nn.Linear(num_channels[-1], output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) #nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Inputs have to have dimension (N, C_in, L_in)\"\"\"\n",
    "        y1 = self.tcn(inputs)  # input should have dimension (N, C, L)\n",
    "        print(y1[:, :, -1].size())\n",
    "        x = self.dense(y1[:, :, -1])\n",
    "        #x = torch.squeeze(x)\n",
    "        # pass through Softmax activation\n",
    "        y = self.softmax(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CQmv48K-xCcs"
   },
   "outputs": [],
   "source": [
    "class simpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(simpleLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).cuda()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).cuda()\n",
    "\n",
    "        out, (h_n, h_c) = self.lstm(x, (h0, c0))\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        #return out\n",
    "        return F.log_softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IMO3YlwCWkN3"
   },
   "outputs": [],
   "source": [
    "class SimpleLearner():\n",
    "    def __init__(self, data, model, loss_func, wd = 1e-5):\n",
    "        self.data, self.model, self.loss_func = data , model , loss_func \n",
    "        self.wd = wd\n",
    "    \n",
    "    def update_manualgrad(self, x,y,lr):\n",
    "        y_hat = self.model(x)\n",
    "        # weight decay\n",
    "        w2 = 0.\n",
    "        for p in model.parameters(): w2 += (p**2).sum()\n",
    "        # add to regular loss\n",
    "        loss = self.loss_func(y_hat, y) + w2 * self.wd\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for p in model.parameters():\n",
    "                p.sub_(lr * p.grad)\n",
    "                p.grad.zero_()\n",
    "        return loss.item()\n",
    "\n",
    "    def update(self, x,y,lr):\n",
    "        #x = x.reshape(-1, 25, 96).cuda()\n",
    "        x = x.view(-1, 1, 2400).cuda()\n",
    "        y = y.cuda()\n",
    "        opt = optim.Adam(self.model.parameters(), lr)\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss_func(y_hat, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        return loss.item()\n",
    "\n",
    "    def fit(self, epochs=1, lr=1e-3):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        losses = []\n",
    "        for i in range(epochs):\n",
    "            for x,y in self.data[0]:\n",
    "                current_loss = self.update(x, y , lr)\n",
    "                losses.append(current_loss)\n",
    "        return losses\n",
    "    \n",
    "    def evaluate(self, X):\n",
    "        \"\"\"Evaluate the given data loader on the model and return predictions\"\"\"\n",
    "        result = None\n",
    "        gt = None\n",
    "        for x, y in X:\n",
    "            x = x.view(-1, 1, 2400).cuda()\n",
    "            #y = y.cuda()\n",
    "            y_hat = self.model(x).cpu().detach().numpy()\n",
    "            result = y_hat if result is None else np.concatenate((result, y_hat), axis=0)\n",
    "            gt = y if gt is None else np.concatenate((gt, y), axis=0)\n",
    "        return result, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_evaluation(validation_generator):\n",
    "    y_pred, gt = learner.evaluate(validation_generator)\n",
    "    result = []\n",
    "    exp = np.exp(y_pred)\n",
    "    for i in exp[:,-1]:\n",
    "        result.append(i.argmax(axis=0))\n",
    "    return result, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "def get_mpca(pred, gt): \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    result = cm.diagonal()\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def get_f1(pred, gt): \n",
    "    f1 = f1_score(gt, pred, average='weighted')\n",
    "    return(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "def get_pca(pred, gt): \n",
    "    cm = confusion_matrix(gt, pred)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    result = cm.diagonal()\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def get_acc(pred, gt):\n",
    "    acc = accuracy_score(gt, pred)\n",
    "    return(acc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "xA-7_XnlXWgR",
    "outputId": "73873740-9e6c-44d5-cc92-f3b8db5703c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_epochs = opt.max\n",
    "max_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LP2gkuBMXyPs"
   },
   "outputs": [],
   "source": [
    "csv_path = {'train':\"/media/data/weiling/Action_Category_CVG/00_datasets/Weiling_data/fake_label_5/S*.csv\", 'val':\"/media/data/weiling/Action_Category_CVG/00_datasets/Weiling_data/fake_label_5/S*.csv\"}\n",
    "npy_path = {'train':\"/media/data/weiling/Action_Category_CVG/00_datasets/Weiling_data/fake_3d_5/S*.npy\",'val':\"/media/data/weiling/Action_Category_CVG/00_datasets/Weiling_data/fake_3d_5/S*.npy\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uv7OKtbffDTW"
   },
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "input_channels = 1\n",
    "channel_sizes = [25]*8\n",
    "seq_length = int(2400 / input_channels)\n",
    "model = TCN(input_channels, n_classes, channel_sizes, kernel_size=7, dropout=0.05).cuda()\n",
    "#model = simpleLSTM(input_size=96, hidden_size=64, num_layers=25, num_classes=10).cuda() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "dACZXae1YSzF",
    "outputId": "bef69f9c-6231-4564-a864-ea1827f53b49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "length of input skeleton is:4973 mod of batch size is:45\n",
      "length of input label is:4973 mod of batch size is:45\n",
      "8\n",
      "length of input skeleton is:4973 mod of batch size is:45\n",
      "length of input label is:4973 mod of batch size is:45\n"
     ]
    }
   ],
   "source": [
    "training_set = my_dataset(csv_path['train'], npy_path['train'], data['Data_hz'],data['Frame_len'])\n",
    "training_generator = DataLoader(training_set, **params)\n",
    "validation_set = my_dataset(csv_path['val'], npy_path['val'],data['Data_hz'],data['Frame_len'])\n",
    "validation_generator = DataLoader(validation_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "z8Zo2F64iBd2",
    "outputId": "81b859bf-3d4e-4f71-9461-70caa9ec88fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True Sequential(\n",
      "  (0): TemporalBlock(\n",
      "    (conv1): Conv1d(1, 25, kernel_size=(7,), stride=(1,), padding=(6,))\n",
      "    (chomp1): Chomp1d()\n",
      "    (relu1): ReLU()\n",
      "    (dropout1): Dropout(p=0.05, inplace=False)\n",
      "    (conv2): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(6,))\n",
      "    (chomp2): Chomp1d()\n",
      "    (relu2): ReLU()\n",
      "    (dropout2): Dropout(p=0.05, inplace=False)\n",
      "    (net): Sequential(\n",
      "      (0): Conv1d(1, 25, kernel_size=(7,), stride=(1,), padding=(6,))\n",
      "      (1): Chomp1d()\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.05, inplace=False)\n",
      "      (4): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(6,))\n",
      "      (5): Chomp1d()\n",
      "      (6): ReLU()\n",
      "      (7): Dropout(p=0.05, inplace=False)\n",
      "    )\n",
      "    (downsample): Conv1d(1, 25, kernel_size=(1,), stride=(1,))\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (1): TemporalBlock(\n",
      "    (conv1): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
      "    (chomp1): Chomp1d()\n",
      "    (relu1): ReLU()\n",
      "    (dropout1): Dropout(p=0.05, inplace=False)\n",
      "    (conv2): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
      "    (chomp2): Chomp1d()\n",
      "    (relu2): ReLU()\n",
      "    (dropout2): Dropout(p=0.05, inplace=False)\n",
      "    (net): Sequential(\n",
      "      (0): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
      "      (1): Chomp1d()\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.05, inplace=False)\n",
      "      (4): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
      "      (5): Chomp1d()\n",
      "      (6): ReLU()\n",
      "      (7): Dropout(p=0.05, inplace=False)\n",
      "    )\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (2): TemporalBlock(\n",
      "    (conv1): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(24,), dilation=(4,))\n",
      "    (chomp1): Chomp1d()\n",
      "    (relu1): ReLU()\n",
      "    (dropout1): Dropout(p=0.05, inplace=False)\n",
      "    (conv2): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(24,), dilation=(4,))\n",
      "    (chomp2): Chomp1d()\n",
      "    (relu2): ReLU()\n",
      "    (dropout2): Dropout(p=0.05, inplace=False)\n",
      "    (net): Sequential(\n",
      "      (0): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(24,), dilation=(4,))\n",
      "      (1): Chomp1d()\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.05, inplace=False)\n",
      "      (4): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(24,), dilation=(4,))\n",
      "      (5): Chomp1d()\n",
      "      (6): ReLU()\n",
      "      (7): Dropout(p=0.05, inplace=False)\n",
      "    )\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (3): TemporalBlock(\n",
      "    (conv1): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(48,), dilation=(8,))\n",
      "    (chomp1): Chomp1d()\n",
      "    (relu1): ReLU()\n",
      "    (dropout1): Dropout(p=0.05, inplace=False)\n",
      "    (conv2): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(48,), dilation=(8,))\n",
      "    (chomp2): Chomp1d()\n",
      "    (relu2): ReLU()\n",
      "    (dropout2): Dropout(p=0.05, inplace=False)\n",
      "    (net): Sequential(\n",
      "      (0): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(48,), dilation=(8,))\n",
      "      (1): Chomp1d()\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.05, inplace=False)\n",
      "      (4): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(48,), dilation=(8,))\n",
      "      (5): Chomp1d()\n",
      "      (6): ReLU()\n",
      "      (7): Dropout(p=0.05, inplace=False)\n",
      "    )\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (4): TemporalBlock(\n",
      "    (conv1): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(96,), dilation=(16,))\n",
      "    (chomp1): Chomp1d()\n",
      "    (relu1): ReLU()\n",
      "    (dropout1): Dropout(p=0.05, inplace=False)\n",
      "    (conv2): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(96,), dilation=(16,))\n",
      "    (chomp2): Chomp1d()\n",
      "    (relu2): ReLU()\n",
      "    (dropout2): Dropout(p=0.05, inplace=False)\n",
      "    (net): Sequential(\n",
      "      (0): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(96,), dilation=(16,))\n",
      "      (1): Chomp1d()\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.05, inplace=False)\n",
      "      (4): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(96,), dilation=(16,))\n",
      "      (5): Chomp1d()\n",
      "      (6): ReLU()\n",
      "      (7): Dropout(p=0.05, inplace=False)\n",
      "    )\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (5): TemporalBlock(\n",
      "    (conv1): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(192,), dilation=(32,))\n",
      "    (chomp1): Chomp1d()\n",
      "    (relu1): ReLU()\n",
      "    (dropout1): Dropout(p=0.05, inplace=False)\n",
      "    (conv2): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(192,), dilation=(32,))\n",
      "    (chomp2): Chomp1d()\n",
      "    (relu2): ReLU()\n",
      "    (dropout2): Dropout(p=0.05, inplace=False)\n",
      "    (net): Sequential(\n",
      "      (0): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(192,), dilation=(32,))\n",
      "      (1): Chomp1d()\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.05, inplace=False)\n",
      "      (4): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(192,), dilation=(32,))\n",
      "      (5): Chomp1d()\n",
      "      (6): ReLU()\n",
      "      (7): Dropout(p=0.05, inplace=False)\n",
      "    )\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (6): TemporalBlock(\n",
      "    (conv1): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(384,), dilation=(64,))\n",
      "    (chomp1): Chomp1d()\n",
      "    (relu1): ReLU()\n",
      "    (dropout1): Dropout(p=0.05, inplace=False)\n",
      "    (conv2): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(384,), dilation=(64,))\n",
      "    (chomp2): Chomp1d()\n",
      "    (relu2): ReLU()\n",
      "    (dropout2): Dropout(p=0.05, inplace=False)\n",
      "    (net): Sequential(\n",
      "      (0): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(384,), dilation=(64,))\n",
      "      (1): Chomp1d()\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.05, inplace=False)\n",
      "      (4): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(384,), dilation=(64,))\n",
      "      (5): Chomp1d()\n",
      "      (6): ReLU()\n",
      "      (7): Dropout(p=0.05, inplace=False)\n",
      "    )\n",
      "    (relu): ReLU()\n",
      "  )\n",
      "  (7): TemporalBlock(\n",
      "    (conv1): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(768,), dilation=(128,))\n",
      "    (chomp1): Chomp1d()\n",
      "    (relu1): ReLU()\n",
      "    (dropout1): Dropout(p=0.05, inplace=False)\n",
      "    (conv2): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(768,), dilation=(128,))\n",
      "    (chomp2): Chomp1d()\n",
      "    (relu2): ReLU()\n",
      "    (dropout2): Dropout(p=0.05, inplace=False)\n",
      "    (net): Sequential(\n",
      "      (0): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(768,), dilation=(128,))\n",
      "      (1): Chomp1d()\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.05, inplace=False)\n",
      "      (4): Conv1d(25, 25, kernel_size=(7,), stride=(1,), padding=(768,), dilation=(128,))\n",
      "      (5): Chomp1d()\n",
      "      (6): ReLU()\n",
      "      (7): Dropout(p=0.05, inplace=False)\n",
      "    )\n",
      "    (relu): ReLU()\n",
      "  )\n",
      ")\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# model summary\n",
    "for m in model.children():\n",
    "    print(m.training)#, m)\n",
    "    for j in m.children():\n",
    "        print(j.training, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U1DThhC7iZun"
   },
   "outputs": [],
   "source": [
    "class_weight=torch.tensor([4.63958011e+01,8.34755467e+00,2.09941000e+04,4.81295277e+00,3.55832203e+00,5.55399471e+01,5.31617331e-01,2.31199824e+00,3.09034438e-01,2.62996624e-01])\n",
    "#class_weight=torch.tensor([51.55064457,9.27501657,5.34769983,3.95367232,61.71075838,0.59068311,2.56887469,0.34336996,0.29221708])\n",
    "loss_func = nn.NLLLoss(weight=class_weight).cuda()\n",
    "#loss_func = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vSrmO0crlYFK",
    "outputId": "32932421-6b9e-4915-f172-15e184b5e595"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66910\n"
     ]
    }
   ],
   "source": [
    "[p.shape for p in model.parameters()]\n",
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "WRckho10lZcL",
    "outputId": "5db13f75-65da-403a-a55a-5b9e8db7061f"
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch import optim\n",
    "lr = 2e-2\n",
    "learner = SimpleLearner([training_generator, validation_generator], model, loss_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f6aNgsuzunYb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n"
     ]
    }
   ],
   "source": [
    "losses = learner.fit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3a339199b0>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd4XNW19/HvmiZp1LtkSZZ7LxgbY2NMNcEEAiH0JCSEYiAQ4JKbm5DkJjckeW+Sm4QeSiC00JxACMU0Y4PBuCB3uVfJqlbvXfv944zGKiNLtmWPZrQ+z6OHmTlHM5sjz2/2rLP3PmKMQSmlVHCx+bsBSimlBp6Gu1JKBSENd6WUCkIa7kopFYQ03JVSKghpuCulVBDScFdKqSCk4a6UUkGoz3AXkVARWSsim0Rkq4j8ysc+N4hIiYhs9PzcfGKaq5RSqj8c/dinCTjPGFMrIk7gcxF5zxizutt+rxlj7uzvCyckJJgRI0YcRVOVUkqtW7eu1BiT2Nd+fYa7sdYnqPXcdXp+jnvNghEjRpCVlXW8T6OUUkOKiOT0Z79+1dxFxC4iG4FDwEfGmDU+drtCRDaLyD9FJKOX51kkIlkiklVSUtKfl1ZKKXUM+hXuxpg2Y8wpQDowW0SmdNvlbWCEMWYa8BHwfC/P85QxZpYxZlZiYp/fKpRSSh2joxotY4ypBJYDC7s9XmaMafLcfRqYOTDNU0opdSz6M1omUURiPLfDgAuAHd32Se1091Jg+0A2Uiml1NHpz2iZVOB5EbFjfRgsNsa8IyL3A1nGmLeAu0TkUqAVKAduOFENVkop1Tfx18U6Zs2aZXS0jFJKHR0RWWeMmdXXfjpDVSmlglDAhfvOohr+9OFOyuua/d0UpZQatAIu3PeV1PLIsj0UVzf6uylKKTVoBVy4u0Osc8D1za1+bolSSg1eARfu4S47AHVNbX5uiVJKDV4BF+5ul/bclVKqLwEX7hGeskyt9tyVUqpXARfu7hCrLKM9d6WU6l3AhXu4pyyjNXellOpdwIV7qNOGiPbclVLqSAIu3EWEcJdDe+5KKXUEARfuAG6XXXvuSil1BAEZ7uEhDuqateeulFK9Cchwd7vs1Ddpz10ppXoTkOEe7nJQp2UZpZTqVUCGuzvEridUlVLqCAIy3K2au/bclVKqN4EZ7i479dpzV0qpXgVkuLu15q6UUkcUkOEeHmKnvrkNf13/VSmlBruADHe3y0Fbu6Gptd3fTVFKqUEpIMO944Id9TqRSSmlfArIcO+41F6dTmRSSimfAjLcw71XY9Keu1JK+dJnuItIqIisFZFNIrJVRH7lY58QEXlNRPaIyBoRGXEiGtuh44IdtdpzV0opn/rTc28CzjPGTAdOARaKyJxu+9wEVBhjxgAPAL8f2GZ21XGpPV0ZUimlfOsz3I2l1nPX6fnpPgbxMuB5z+1/AueLiAxYK7txe06o6hIESinlW79q7iJiF5GNwCHgI2PMmm67pAEHAYwxrUAVED+QDe3scM1de+5KKeVLv8LdGNNmjDkFSAdmi8iUY3kxEVkkIlkiklVSUnIsTwEcrrnrmu5KKeXbUY2WMcZUAsuBhd025QMZACLiAKKBMh+//5QxZpYxZlZiYuKxtZhOPXc9oaqUUj71Z7RMoojEeG6HARcAO7rt9hbwXc/tK4Fl5gSuDRDm1J67UkodiaMf+6QCz4uIHevDYLEx5h0RuR/IMsa8BTwDvCgie4By4NoT1mLAZhO9GpNSSh1Bn+FujNkMzPDx+C863W4ErhrYph2ZtTKk9tyVUsqXgJyhCh0rQ2rPXSmlfAnYcHe7HLq2jFJK9SJgwz1Cr6OqlFK9Cthwd7scWpZRSqleBGy4h4fY9YSqUkr1ImDD3e1y6FBIpZTqRcCGe7hLe+5KKdWbgA13d4jW3JVSqjcBG+7hLjstbYZmvUi2Ukr1ELDh7tZlf5VSqlcBG+7heqk9pZTqVcCGe0fPXScyKaVUTwEb7lFhTgCqG1v83BKllBp8Ajbc49wuACrqmv3cEqWUGnwCNtxj3FbPvbJee+5KKdVdwIZ7bLjVcy+v1567Ukp1F7DhHu6y47QLFRruSinVQ8CGu4gQ43ZRWadlGaWU6i5gwx2sk6rac1dKqZ4COtxj3E49oaqUUj4EdLjHul16QlUppXwI7HAPd1Kp4a6UUj0Edri7XVTWt2CM8XdTlFJqUAn4cG9tN9To4mFKKdVFQIe7d5aqDodUSqku+gx3EckQkeUisk1EtorI3T72OUdEqkRko+fnFyemuV3FunWWqlJK+eLoxz6twA+NMetFJBJYJyIfGWO2ddvvM2PMJQPfxN7Fhls9dx3rrpRSXfXZczfGFBpj1ntu1wDbgbQT3bD+6Oi564gZpZTq6qhq7iIyApgBrPGxea6IbBKR90Rk8gC0rU+x3mV/teaulFKd9acsA4CIRACvA/cYY6q7bV4PZBpjakXkq8CbwFgfz7EIWAQwfPjwY250h6gwJyLac1dKqe761XMXESdWsL9kjHmj+3ZjTLUxptZzewngFJEEH/s9ZYyZZYyZlZiYeJxNB7tNiA5z6glVpZTqpj+jZQR4BthujPlzL/ukePZDRGZ7nrdsIBvaG2vxMC3LKKVUZ/0py8wDrge2iMhGz2M/BYYDGGOeAK4EbheRVqABuNacpGmj1uJh2nNXSqnO+gx3Y8zngPSxz6PAowPVqKMR63ZRWNXoj5dWSqlBK6BnqALWBTu0566UUl0EfLjHuvWEqlJKdRf44R7uorGlncaWNn83RSmlBo3AD/eOiUzae1dKKa8gCHfP+jI6S1UppbwCPtxjdH0ZpZTqIeDDPS5cl/1VSqnuAj7cvWUZnaWqlFJeAR/u3rJMnfbclVKqQ8CHu8thI9xl1567Ukp1EvDhDjpLVSmluguKcI8Ld+kJVaWU6iQowj3G7dSyjFJKdRIU4R6rZRmllOoiSMLdSYWOllFKKa/gCPdwF9WNrbS2tfu7KUopNSgER7h3jHVv0Lq7UkpBkIR7jGeWqtbdlVLKEhThfnjZX+25K6UUBFu460lVpZQCgiXcwzsWD9NwV0opCJZw17KMUkp1ERTh7nbZcdlt2nNXSimPoAh3ESHG7aRSL7WnlFJAkIQ7WKUZ7bkrpZSlz3AXkQwRWS4i20Rkq4jc7WMfEZGHRWSPiGwWkVNPTHN7Fxvu1HBXSimP/vTcW4EfGmMmAXOAO0RkUrd9LgLGen4WAY8PaCv7weq5a1lGKaWgH+FujCk0xqz33K4BtgNp3Xa7DHjBWFYDMSKSOuCtPQK9YIdSSh12VDV3ERkBzADWdNuUBhzsdD+Pnh8AJ1Ss20llfQvGmJP5skopNSj1O9xFJAJ4HbjHGFN9LC8mIotEJEtEskpKSo7lKXoVF+6itd1Q09Q6oM+rlFKBqF/hLiJOrGB/yRjzho9d8oGMTvfTPY91YYx5yhgzyxgzKzEx8Vja26sYXYJAKaW8+jNaRoBngO3GmD/3sttbwHc8o2bmAFXGmMIBbGefYt0dSxDoSVWllHL0Y595wPXAFhHZ6Hnsp8BwAGPME8AS4KvAHqAe+N7AN/XIosKscK9p1HBXSqk+w90Y8zkgfexjgDsGqlHHIjLU+l+pbtCau1JKBc0M1ahQ7bkrpVSHoAn3jp57TaP23JVSKmjCPdzlQASqteeulFLBE+42mxAZ4tCeu1JKEUThDhAZ6tSeu1JKEWThHhXm1NEySilFkIV7ZKhDR8sopRRBFu5RoVpzV0opCLpw15q7UkpBkIV7pPbclVIKCLpwd1LTqGu6K6VUUIV7VJiDdgN1zW3+bopSSvlVUIV7pK4vo5RSQNCFu64MqZRSEGThritDKqWUJajCXVeGVEopS5CFu9Vz17HuSqmhLqjCPSrMU3PXnrtSaogLrnDXmrtSSgFBFu4hDhtOu+hoGaXUkBdU4S4iRHlmqSql1FAWVOEOur6MUkpBUIa7rgyplFJBF+5RYdpzV0qpoAv3yBCtuSulVJ/hLiJ/E5FDIpLdy/ZzRKRKRDZ6fn4x8M3sv6gwh46WUUoNeY5+7PMc8CjwwhH2+cwYc8mAtOg4RepoGaWU6rvnboxZAZSfhLYMiMhQB3XNbbS2tfu7KUop5TcDVXOfKyKbROQ9EZk8QM95TDpmqdY2aWlGKTV0DUS4rwcyjTHTgUeAN3vbUUQWiUiWiGSVlJQMwEv3pCtDKqXUAIS7MabaGFPrub0EcIpIQi/7PmWMmWWMmZWYmHi8L+1Tx8qQVQ1ad1dKDV3HHe4ikiIi4rk92/OcZcf7vMcqOsyz7K+Gu1JqCOtztIyIvAKcAySISB7wS8AJYIx5ArgSuF1EWoEG4FpjjDlhLe5DXLgLgIp6DXel1NDVZ7gbY67rY/ujWEMlB4VYt9VzL69v9nNLlFLKf4JuhmqM2+q5V9ZpuCulhq6gC3eXw0ZkiEN77kqpIS3owh0gJtxJhfbclVJDWFCGe5zbpSdUlVJDWlCGe2y4iwotyyilhrCgDPc4t4tyLcsopYawoAz3GLdLa+5KqSEtKMM9LtxJXXMbTa1t/m6KUkr5RVCGe6xnlmqlnlRVSg1RQRnucZ6JTFp3V0oNVUEZ7h2zVHXEjFJqqArKcPcuHlanZRml1NAUlOEeG66LhymlhragDPeYMF08TCk1tAVluOviYUqpoS4owx08SxBoz10pNUQFdbiX6zh3pdQQFbzh7nZSqWUZpdQQFbThrouHKaWGsqANd625K6WGsqAN97hwly4eppQasoI23GPc1kQmXTxMKTUUBW2493fxsPK6Zs794ydsL6w+Gc1SSqmTImjDvWPZ377CfXdxDftL69iQW3kymqWUUidF0IZ7QkQIAKW1TUfcryP8i6sbT3ibevPK2ly25FX57fWVUsGnz3AXkb+JyCERye5lu4jIwyKyR0Q2i8ipA9/Mo5foCfeSmiOHe6kn3A/V+CfcjTH8z1tbeXH1Ab+8vlIqOPWn5/4csPAI2y8Cxnp+FgGPH3+zjl9UmAOX3UZJHz33Ms/24uoj73ei1DS10tTaTmmtDttUSg2cPsPdGLMCKD/CLpcBLxjLaiBGRFIHqoHHSkRIiHBRWtP3CVXwX1mm1PPNoq/ykVJKHY2BqLmnAQc73c/zPOZ3CZEhfYZmWW1HuPsnXDt67KV9lI+UUuponNQTqiKySESyRCSrpKTkhL9eYkRInzX3srom739b29pPeJu66/jwKa1txhhz0l9fKRWcBiLc84GMTvfTPY/1YIx5yhgzyxgzKzExcQBe+sgSIvrfczcGv9S9O9rX3NZOdUPrSX99pVRwGohwfwv4jmfUzBygyhhTOADPe9wSIl2U1TXT3t57j7i8rpmUqFDAP3X3zuWYvk7+KqVUf/VnKOQrwCpgvIjkichNInKbiNzm2WUJsA/YA/wV+P4Ja+1RSowIoa3dUNHL0r9t7Yby+mYmDYsC/BPuJZ2+LfRVQlJKqf5y9LWDMea6PrYb4I4Ba9EASojsmMjUTLxn3HtnFfXNGAMTUyNZtuMQxX4I19LaJlx2G81t7QM2YqawqgGbCMmebyRKqaEnaGeoQt+zVDuGQY5LjsQmcMgfZZnaJsYmR3hvD4S7X9nIj/65eUCeSykVmII63BMjfc9SLahswBjjDdPEyBASIkL8U3OvbWJMUgQOmwxYWWbXoRp2F9cMyHMppQJTUIe7r577O5sLOON3y1i5p8zbc0+ICCE5KtQvY91La5pJjAghPsI1ID33qoYWKutbKKxqpLFF17JXaqgK6nCPCu26BMGhmkZ+/qa1RE5WTrl3GGRcuIvkqMM99x1F1bQdYYTNQKlraqWhpY0EzzeHgRiKebC83udtpdTQEtThLiIkRloTmYwx/PSNbBqa20iMDCE7v4qy2iZEINbtIikqlJKaJtbsK2Phg5/x9qaCE96+jjJMQkSIt53HK6fscKAfKNNwV2qoCupwB6z1ZWqbWbWvjKXbi/nPr4znzDEJbMmvoqyumTi3C7tNSI4MpayumYc+3g3A+tyKE9627jX/3soyi7MO8sWe0n49Z26n3npOWd3xN1IpFZCGQLhbPeK3NxXidtn59pxMJg+Lori6iR1FNcR5LuqRHGXV57/YW4YIbB7A9dVfWZvL3pLaHo93hHlChMsb7t2XIGhta+eX/97Kb97d3q/Xyi2vIz7cRXSYs0svXik1tAR9uCdGWrX097MLWTAxmTCXnalp0QBsyK0gPsIK9yRPuEeEOLh6ZgbbC6tpGYC1ZoqqGrnvjS38ZfneHts6JjAlesoyLW2Gqoau13zdUVRDQ0sb2wqrOVDad088t7yejDg3mfFuDmjPXakhK+jDPSEihPK6ZirqW7h4mrUS8eS0aESg3UB8uBXqKVFhAHx7TibzxibQ1NrO7uKeve2jtWqfVU5Zuae0R6+8tMaq+ceFu0jwfMiU1jbx99U5fHnAWmV5Q6fy0JLsvld1yCmrJzPeTWZ8uPbclRrChkC4W6EZEeLg7HGJ3tsjE8IBvD33iamR/N+V07jzvDHenv2W/MPXVW1saeOOl9ezteDI5Zr2dsN9b2whyxPOq/aWAVBU3ci+bj3v0tomYt0uHHab98pRn+8u5edvZvO793YAsD63ksTIEKZnxLBky5HDvbm1nYLKBjLj3GTGucmvbBiQbx9KqcAT9OGeGGlNwb9gUjKhTrv38Y4A76i5iwhXzcogIsRBZpybyFAHW/IPB/k7mwt5d3MhL3yRc8TXW3ugnFfW5vKnD3cBVg1/Uqq1ds3KbidFS2ubvB8+HROuHlhqndBdl1NBQWUD63MrOHV4DBdPTSE7v5qcsjrW7Csj10evvKCygXaDtyzT1m7Ir2jo55E6dlX1Lews0klTSvmSnV9FTWNL3zsOsKAP98x4NwBfn9H1+iFThlnh7mvNGZtNmDIsustFq19dmwvA0u3FtLUbjDF8uLWox/DFf2+0hlCu2lfGp7tKyKto4JrTMsiIC+Pz3d3Dvdk70arjv1UNLXx1agoAL63JIaesnhnDY7loilVSuvjhz7nmqdVc/peV7DnUtWyU4xkpkxkfzgjPN5MTXXevamjhqie/4GuPfE5+5Yn/IAlkxhh+uHgTy3cc8ndT/GZfSS1PfLp3yFy7oKm1jW88/gV//Wz/SX/toA/3KWnRfPZf53pLMh2mpVvhnhTZM9w7tm8vrKG5tZ1dxTVk5Vg96LK6ZtblVPDF3jIWvbiOa55c5V2Tprm1nSVbCjlzTAIOm/DTN7YAcMboeOaNTmDVvrIuk6Osnrv1+tFhThw2Icxp59eXTWFiahRPe/5BnDo8low4NxdPS2ViaiS/vmwyIsK3nl7dpQef6wny4Z6yDHBC6+5NrW3c+mIW+0vrMBge/2RPl+3GGJpbe5aFVu0t497FG2lq9T2DtqqhhWU7iv0WAC1t7Ty6bPeAX/pwa0E1r6/P42VPR2Eo+vvqXH733o4h0xEoqGykubWdnUXVJ/21gz7cwSpTdDd7ZByPffNUzh2f5PN3pqZH09zWzpb8Sl5Zm4vTLjx4zQxcdhsfbC3ioaW7iQ93UVTdyHV/XU1RVSMrdpVQ1dDCTWeO5IJJyeRXNpAQEcKYpAjmjUmgprHVW+rJq6inoLKBYTHWiVybTZg7Op47zh1NfEQIF09Noam1HYdNvB9Ej33zVP5x2xlcP3cEf795Nk2t7Vz5xBdszrPODeSW1xPisJEUaY2+CXPae/Tcj7S2/dH604e7WL2vnD9eNZ2rZ2Xw2pcHKfC8aQ+W13PNU6s58/fLuoR4XVMr9y7eyBvr8/nX+p7XdKlvbuWGZ9dy43NZvPrlwR7bO1TVn7ivuf/eWMAfP9zFS6sHNoTf85wQ//JA+YD+HQJJtuecVXb+yQ87f8irsDpXe0tO/si1IRHuvogIF09LxeXwfQhmDI9FBK58YhUvrsrhK5NTGB7vZt6YeF5Zm8vaA+X84LwxPHvDaRRUNnLhgyt4YOkuYt1OzhybwDdPHw7A3NHxiAhnjI4H4LUvD2KM4f8t2Y7dJnxnbqb3NV+86XTuPG8sAF+dapVhJg2L6nKuoMOElCheWzQXp93G1U+u4rfvbuOz3aUMj3NjswkiQma8m7X7y6lpbKGoqpGvPfI5d7y8vt/HaNPBSh5bvoc1+8qobmyhqbXN25uuqGvmxVU5fGNGGpedksb3zx0DwH1vbOFn/9rCRQ99RtaBcg7VNHWZM/DQx7sprGokLSaMxz/d2+XShi1t7dz58gY2HaxkbFIE97+9jf0+hn/+7r0dzPzNR2wr6BoQTa1tLNlS6PPbAkBuWT03PLuWb/51Nbe9uI4Hl+7i4+3FrMupYM+hWoyxym1/XbEPgGU7igHrZPov/51Ndv6xz30wxvDeliJcdhuV9S3sPnT8I7ECTXu78f7NjudYBpI8zzmvnLK6kz64oc/13IeqtJgw3rt7Ph9uLSYrp4LvnzMagAsnp7B8ZwlJkSFcO3s4oU4779x1Jvcu3sSmg5V8e85wnHYb80Yn8K3Th3tr/fERIXxv3gieXXmAmsYWlmwp4t4Lxnl77t2NSozg4qmpzBoR22sbx6dE8u8753HXKxt4duUBWtsN18w6fMXDm+eP4sevb+ayR1fS2NJGQVUjuw/V0NLWjtNuo66plUM1TaTFhGETKKxqJDLUQYzbOsl7/zvbWJfTdabu+ORI/n7z6by8JpeGljZu8xyXtJgwrps9nBdW5RAZ4mDu6HjuPn8slzzyOWv2lXHaiDh2FFXzzOf7ufa0DM6dkMStL67jnc2F3mP00uoclu04xG++PoUFE5O58MEV3PPaRt64/QzsNgHgb5/v54lPrTkD/1h3kF8Om+xt2xOf7OOBpbu4aEoKj1w3A4f98Ad3Q3Mbi17MIr+igQmpkew+VMMH24roXPm5cd5IzhqXwM7iGiakRLIpr4qSmiY+3VXC86tyWJJdxFt3ziM12vffrDtjDH/+aBdzR8cTHx7CvtI6bj17FE9+uo+1+8sYnxLZr+cJFgfK6qhtsi4lmd3HqLNA1d5u+OVbW7lqVjrT0mO8PfeWNkNueT2jEyNOWls03I9gQkoUE1Kiujy2YFIyEe9u5+4FY7096tGJEbx+21yWZBcxf0wCYJVZfnv51C6/+98XT6KqoYU31ueTHhvGorNGHfH1H/vWqX22MSEihJdvmYMxhqbWdkI6fRO5cmY66bFh3PnyBkTg9nNG8/gne9lRWMPU9GjuXbyRD7YWIwJ2EVrbDSPi3Xz8w3Moq21iXU4Ft541ipmZsRwoq6OuqY0nV+zlxue+JK+ingUTkxiXfDig/vuSSSw6axTDosOwecJ4Qkoka/aXcyfw9Gf7CXPa+fHCCUSHORmXHMFjy/dw6fRh2GzCvzYWMHlYFN+eY32b+fHCCfz0X1vYVlDN1PRo1uVU8Ot3t7FwcgrtxvD2pgJ++tWJOO02SmubeGrFXjLiwngvu4ifvLGFP1wxDZtNMMbwkzc2s7O4hmdvOI1zPKW42qZWdhRWU9vUykfbivnbyv38c91BkqNC+P0V07jssZUs33mIl9fkkhYTRmV9M4teWMfiW+cS5rL+9sYYRMTn3+aDrcU8smwPT63Yx7wxCYjATWeO5O2NBazZX871c0f0+fc9VruLaxCBMUmD5wMk29Nrn5IWRXZ+1RGPXaDKLa/nxdU52G3iCffD5xb2HqrVcB/MEiJCyPr5gh6lEofdxqXThx3xd2024Q9XTGN4nJuzxiX6LLccKxHx+XxzRsWz/D/PxgDVDS08/sleNhysYFxKBCt2lXLWuEROyYihta2dxpZ2/rZyP5/uOuQdQnnlzHTGdgrwqWnRLHoxi3ZjfVh05rTbSI/ten7j9JFxLM7Ko7qxhfezi/jq1BRiPcNP7zh3DHe/upEPtxVbPeWDldx30QTv784bY5WysguqmJoezac7D2ET4Y9XT+eLPaV8uK2Yz3aXcN6EZB5dtofG1nae+95s3t5UwINLd1NZ38xvL5/Kb97dztubCvjPr4zzBjtY8x1mjYgD4OxxiThswvOrcrj9nDFMS48mNTqUv67Yx+5DtfzikkkMj3Nzy4tZXPH4FzzyzRl8vruUB5fu4pazRvH9c6yy1LaCakYkuAl12Hno492MiHcT4rCzbMchZo+IIykylNkj41i5t6xHuLW1G77+2ErOn5jEPQvGAdZJehHr2Bpj2H2olqTIEO+3K1+qGlq45qnVNDS38dz3TuP0UfG97nsyZedX4XLY+Popafzm3e0UVTf2+1tQoNhWaH2AdQwNzqtoYPKwKLYWVJ/0uruG+zE4nlB22G3eN+7JEhnqtP4b4iApMoQNuZWMSYygoaWN78zJZMGkZMCqeb+9uYC/r86lsaWNUYnhjEnq2tNYMCmZh6+bwfbCamZmxvX52qePiuf5VTk88NEuaptauwxJvXhqKn/+aBePLd/DBZ42XNLpA3K4Z75BR312Y14VY5MiiAhxcM74JGLdTl5fn09TSzsvrcnhmtMyGJ0Ywd3njyXW7eL+d7Zxxu+WYYzhvxaO5/azu34YdSYi/M+lk7l42jBmZsYiIpw3IYmX1uQS6rRxxanpRLudPPPdWdy7eBML/vwpxkBKVCh/eH8nKVGh7Cyq4ckV+xiTFMFVM9PZXljNA9dM58wxidy7eCPf8fTUZ4+M582NBXywtZiX1uTwtWnDuPq0DD7YWsSW/CryKuq5/ZzR2EVY+NAK8isamDQsiqKqRgqrGpmQEsmbd8zr9d/hIx/vpqK+mfTYML733Je8cONs74eYP2XnVzExJZIZw2M996tPSLgbY9hZXMOX+8tBhOgwJxdPTfWW9k6kjnMKO4trMMaQV1HP/LGJlNQ0+Vxf6kTScB9CRIQZw2PYkFtBUlQITrs1QqeD027jutMyeGT5Hmwi3Hb2KJ9fmy+ZNoxLph35W0qH2SOtUHn+iwOkRocyZ+Th13PYbXz/nNH8+PUt7C+tY/aIONI6nYMQseYbZBdUY4xhc14lCydbcwBcDhtfmz6MF1bl8O7mQkYnhnPPgrHe3/vuGSMYlxzJY8v38P1zR3PG6IR+HZ+O9gKcP9EK969NG0a02/qAPG9CMu/eNZ8/fbCTs8cnsnANWm6aAAASBElEQVRKCt95Zi33Lt4EwNemD+Pz3SX873s7GJUYzqXT07DbhBdvOr3HMbnt7+sAyDpQwZljE3hqxT7CnHYq6ltYuu0QLW3t7Cup42vTh1Fc3ci09GiumpnOw8v28Ku3t/G/35jK/tI6YsKc3m9De0tqee6LA1w9M4MffmUcVz25ip+/mc3795zlff32dsMDS3fR3NbOTxZO6PI3bmxpo7yumdTo0KMqmazcU8rOohpuPHOkz+3GGLLzq7hk+jAmpUZhE9iSX+X9UO9uR1E1T3yyl99fOY0QR/87U3kV9Vz/zNoeJ+IdNvEOUjiROnru5XXNFFQ1UlzdRHpsGKMTIzTc1Yk1Y3gsH2wt5t3NhczMjCU8pOs/gWtnD+fR5XtoazfeiVPHIyEihLFJEew+VMtlp6R5a/EdLp+RzkNLd1NQ1cjXTun5gTElLYrnV+Wwt6SOyvoWpmfEeLfdcMYI9hyq5fIZaVw+I63LCVSwRip1/vA6WvPGJPCduZnc1C2w0mLC+PM1p3jvP3n9TH78+mbOn5jM1bMyyKuo59fvbOP6OSN89hZHJ4Yze0QcydGh3DJ/JFc/uYobn/uSHUU1/M/XJvHUin28+mUuh6qbGJccwUPXnNLluLW0Gx7/ZC9r9pexr6SO6RkxvPn9MxARfvvudkKddv7zwvEkRobw7dMz+e2S7eRV1JMe66alrZ0fLt7EW57rFSRHhnYJ5HsXb2TJliIiQx2cOz6J+y+bfMQSEMC7mwu5+9UNtLYbLpiU7HPo8cHyBqobW5kyLJowl50xSRFHHDHzxCd7eXNjAdfNHn5UZaXFWXkcKKvjt5dP4exxibjsNr768Oe8u7nw5IR7QTUpUaEUVTeyzDNZLT3WTWlSE29tLDip5xmG7FDIoWqGJxzzKhqYPzaxx/ZhMWFcNCWVUYnhTB4W1WP7sTh9lNVT/capaT22uRw27l4wlqhQBxf7ePNNSYumubWd19fnAYcnn4E1oujlW+Zw1ayMHsE+EEIcdu6/bAqZ8eFH3C/G7eLJ62dxtWekUnqsmyevn8WZY31/WxARFt82l0eum8G09Bh+cN5YdhTVEON2cs1pw7lyVgaf7S5lZ3ENt58zuscH4g8vGMeCiUnEhDm54tR0Nh2s5MNtxXy6q4RlOw7xg/PGeJezOG+idY6hY1bsz/61hbc2FfCjC8dzwaRkfrtkO2v2edY/qmrkg63FLJiYxCXTUnkvu5BLHrGC8f3soh7rKrW1G57+bB8/eGW9d+TPe70sbtcxOqZj2Y8padG9hnt1YwvvZRcBsOFgpc99evPh1iJOy4zjW6dnkh7rJikqlIVTkvl4RzH1za2U1jZx3xtbvKNYBlJ5XTNF1Y1cNsPqpCzbbg2l7ei5Vze2DsjV1vpLe+5DzNT0aOw2oa3d9Ji12+FPV0+nqbV9wHoYt541mmlpMV1G1nR2zWnDuXJmhs9e7hRPGCz+8iChTluvzxHIbp4/khW7SvjK5BTCXHaumpnOI8t2Myw6zGf5y2G38fR3TwOs9f435Fbwpw930m6s5TZumDfCu+/oxAhGJoSzdPsh5o9N5J/r8rhx3kjuOHcM1Y0tfP3Rldz16gY+uvdsXvvyIG3thv++ZBKZ8eFcPSuDO15a750bIQK3nz2a288Zzaq9ZTy8bDfZ+dUsmJjEw9fN4JonV7NkSxGLzhrNoZpGdhfXMs8zeuyLvaWEOGyMS7HO4ZySEcMb6/M5UFrnXSqjw7ubC2lqbSfMaWdjbv/DPaesjh1FNfz84oldHr946jD+vjqX5TtK+HhHMW+sz+dgeT0v3jT7uP6Nr95XxrT0aNwuK0a3e0oy88ck8vq6PFZ6Fg1Mjw3zzr3YW1Lr/eA90TTchxi3y8GElEiKqhq9C5p1F+q0D+hInow4t8+v6p31drJrZHw44S47ZXXNzMqMxXkCeuj+FuKw89qtc733M+Lc3HfRBManRPX5/+uw27jngnHc9coGAJ66fmaPGvV5E5J4cVUODy7dhcNm47azrSG4UaFOHrjmFC7/y0p+8842Pt9dyvyxCd5vKjOGx/LBf5zFruIaQhx2/r46h798spfHP92LMdbSHY9+cwYXT01FRLhoagp/eH8nOWV13PHyerLzq1l671mkx7p5a2MBF01J8bbNmhm+laXbi7l5ftchwf/IOsjYpAgmDYtilY9RRb35cKvVU77Qc16mw+yRcSREhPDwx7vZWVzDpNQoPt9Tyj/W5Xm/bdU2tfLx9mKWbClk1d4yfnTh+CMOVf10Vwnf/dtafnTheO449/BIKbBWmB2fEsnKPWXYbUJK1OHzF3tLaplzkkYvabgPQfddNJHaptYeX/cHI5tNmDwsmrUHyrvU24PdorN6H9nT3SVTU3nhiwPEhrt8nqA8f2ISz3y+31vDTooK9W6bnhHDLfNH8aRnVu7PL5nU5XcjQ53eUVG/u2Ia54xPYsPBCs4am8hpI+K6zPC+aEoqf3h/J9979kv2ldbhsIl3jH91Y6s3SMH6ABufHOkNd2MMWwuq2VZYzfpca0hsqNPOvzcWUFjVSF1TKw8v28NvL59ClGf0V3cfbC1iYmpUj46E3SYsnJLM31fnkhodyuLb5nLjs1/ym3e2sXpvGeX1zazaW0ZTaztJkSGMSAjnv/+9lZY24/MEcXNrO796eysAK3aVHA73QqveHh8RwvjkKFbuKSM1OhSH3UZqVCgJES7ezy7iW6dn9njOE6Ff4S4iC4GHADvwtDHmd9223wD8H9CxWMijxpinB7CdagD1VgserCanRbH2QHmXers6zGYTXl00B7tn2YnuThsRR2Sog7qmVm71MXHuPy4Yx4fbiqlpbO119EqHhVNSWDglxee2kQnhTEyNYnthNVecmk5EiJ2X1+aytaCa9NiwHj3W8ycm8eSKfVTVt/D8qgP8+SNrmWy3y87lM9Io8izItyG3kjc35vPRtmKGxYRy30WHyy7Z+VX8e2M+LoeNdbkV3H3+WJ9tu3xGGi+tyeXnF08iIsTB766Yyh0vb2DtgXLcLjvXzR7OxdNSmTk8ltZ2w12vbOD+d7axJb+K288Zzevr8njuiwPMGRXP6MQI9pXUMS09mvW5FdQ3t+J2OdhWUM3EVKtsON5TfkqPPbx21C3zR/G/7+1gXU4FMzN7n3k+UPoMdxGxA48BFwB5wJci8pYxZlu3XV8zxtx5Atqohrg5o+J5aXXuoBirPVgd6YSy027jlvmjqG9u61HfBqsM99LNp1Pf3HbcZa9vzxnOS6tz+eWlk6iqb+HF1TlsLajmngVje3xTXDApmb98spcXVh3g0eV7+MqkZO5eMNYzv8FJjNtFiMPGq1/m8tnuUmLcTp5deYBvn55pzQB/dxtLthThtFuzqx026XWI7szMOLJ+tsC7xPeoxAjeu3u+z31dNuGRb87gwaW7ePqz/fxrg9VnvWBSMqs9S3mfNyGJG84YwXf+tpY1+8uZmRnL3pJazvecwB7vmdneeVLf9XMzeWrFPh5cuqvL0NgTpT8999nAHmPMPgAReRW4DOge7kqdEF+ZlMyan57vHcutjt5dvfRoO/S2xtHR+tbpmd6yQ1Sok4unDeOdzQVccWp6j31PSY8hIcLFnz7aRbjLzq+/PoXkTiUjl8PG1LRoPttdSrjLzquL5nDZoyu58+X17Cupo80Y7jp/LDfPH0mEy0FLe/sRx8T7unZDb5x2Gz+6cALfOj2Tf23I58wxCUzPiKG8rpl/ZB3k6zPSiA5z4nLYWLm7lOy8KlrbjXe45bjkCEKdti6TAN0uB7eePYr/t2QHWQfKT3hnpT8f02lA57VX8zyPdXeFiGwWkX+KSIaP7UodExHRYA9Qv7p0Mi/ddLrPE+o2mzULGKwPn87B3mHGcOs8y7fnZjIhJYpb5o9iU14VE4dF8cE9Z3HvBeOICnVis8lRTXbqr2ExYdxx7hjv+Z64cBe3nj2a5KhQQp12ZmXGsmzHIZ5ZuZ/zJiR5R3e5XQ7ev/ssbjhjRJfn+/acTBIiXHywtWjA29rdQJ1QfRt4xRjTJCK3As8D53XfSUQWAYsAhg8fPkAvrZQarOLCXZwxpvdzPDeeOZIwp53vzfM9s3XhlBRW7inj5jOtcwV3LxjLvDEJnD4yblAMCDhzbAJ/eH8nAD84b0yXbb5KYG6Xg3fvmt/rRYIGkvR1tRsRmQv8jzHmQs/9+wCMMf/by/52oNwYc8SzX7NmzTJZWVnH1GillBoMNudVcumjK5k/NuGk1NEBRGSdMWZWX/v1p+f+JTBWREZijYa5FvhmtxdLNcZ0TE27FNh+lO1VSqmAM2VYNLefM5pvzPBVqfavPsPdGNMqIncCH2ANhfybMWariNwPZBlj3gLuEpFLgVagHLjhBLZZKaUGBZtN+PHCCX3v6Ad9lmVOFC3LKKXU0etvWSb45nIrpZTScFdKqWCk4a6UUkFIw10ppYKQhrtSSgUhDXellApCGu5KKRWE/DbOXURKgJxj/PUEoHQAm3MiDPY2avuO32Bvo7bv+A3GNmYaY3xfI7MTv4X78RCRrP4M4venwd5Gbd/xG+xt1PYdv0BoY2+0LKOUUkFIw10ppYJQoIb7U/5uQD8M9jZq+47fYG+jtu/4BUIbfQrImrtSSqkjC9Seu1JKqSMIuHAXkYUislNE9ojITwZBezJEZLmIbBORrSJyt+fxOBH5SER2e/4b6+d22kVkg4i847k/UkTWeI7jayLi14uUikiM5/q7O0Rku4jMHUzHUET+w/P3zRaRV0Qk1N/HUET+JiKHRCS702M+j5lYHva0dbOInOqn9v2f52+8WUT+JSIxnbbd52nfThG50B/t67TthyJiRCTBc/+kH7/jFVDh7rmE32PARcAk4DoRmeTfVtEK/NAYMwmYA9zhadNPgI+NMWOBjz33/eluul4h6/fAA8aYMUAFcJNfWnXYQ8D7xpgJwHSstg6KYygiacBdwCxjzBSsi9Zci/+P4XPAwm6P9XbMLgLGen4WAY/7qX0fAVOMMdOAXcB9AJ73zLXAZM/v/MXzfj/Z7UNEMoCvALmdHvbH8Ts+xpiA+QHmAh90un8fcJ+/29Wtjf8GLgB2Aqmex1KBnX5sUzrWG/084B1AsCZmOHwdVz+0LxrYj+ccUKfHB8UxBNKAg0Ac1tXL3gEuHAzHEBgBZPd1zIAnget87Xcy29dt2+XAS57bXd7LWFd+m+uP9gH/xOpgHAAS/Hn8jucnoHruHH6TdcjzPDYoiMgIYAawBkg2h68rWwQk+6lZAA8C/wW0e+7HA5XGmFbPfX8fx5FACfCsp3T0tIiEM0iOoTEmH/gjVk+uEKgC1jG4jmGH3o7ZYHzv3Ai857k9KNonIpcB+caYTd02DYr2HY1AC/dBS0QigNeBe4wx1Z23Geuj3i/DkkTkEuCQMWadP16/nxzAqcDjxpgZQB3dSjB+PoaxwGVYH0LDgHB8fJ0fbPx5zPoiIj/DKmm+5O+2dBARN/BT4Bf+bstACLRwzwcyOt1P9zzmVyLixAr2l4wxb3geLhaRVM/2VOCQn5o3D7hURA4Ar2KVZh4CYkSk4wLp/j6OeUCeMWaN5/4/scJ+sBzDBcB+Y0yJMaYFeAPruA6mY9iht2M2aN47InIDcAnwLc8HEAyO9o3G+gDf5Hm/pAPrRSRlkLTvqARauH8JjPWMUnBhnYB5y58NEhEBngG2G2P+3GnTW8B3Pbe/i1WLP+mMMfcZY9KNMSOwjtcyY8y3gOXAlf5uH4Axpgg4KCLjPQ+dD2xjkBxDrHLMHBFxe/7eHe0bNMewk96O2VvAdzyjPuYAVZ3KNyeNiCzEKhFeaoyp77TpLeBaEQkRkZFYJy7Xnsy2GWO2GGOSjDEjPO+XPOBUz7/PQXH8joq/i/7HcALkq1hn2fcCPxsE7TkT66vvZmCj5+erWHXtj4HdwFIgbhC09RzgHc/tUVhvnj3AP4AQP7ftFCDLcxzfBGIH0zEEfgXsALKBF4EQfx9D4BWscwAtWEF0U2/HDOsk+mOe980WrJE//mjfHqzadcd75YlO+//M076dwEX+aF+37Qc4fEL1pB+/4/3RGapKKRWEAq0so5RSqh803JVSKghpuCulVBDScFdKqSCk4a6UUkFIw10ppYKQhrtSSgUhDXellApC/x8EDzcbo9OJzgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], array([8, 8, 9, ..., 9, 8, 9]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "torch.Size([64, 25])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(real_evaluation(validation_generator))\n",
    "print(len(real_evaluation(validation_generator)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
